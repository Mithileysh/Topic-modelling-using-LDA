{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA analysis of 'sent' subset of Enron emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full Enron corpus was downloaded, and an attempt was made to filter through the Inboxes of each of the 150 employees. A clear problem emereged, namely, that the emails were vastly inconsistent. The following [PDF](http://www.colorado.edu/ics/sites/default/files/attached-files/01-11_0.pdf) was found that shed light on analysing the Corpus. The observation was made that the 'Sent' directory would contain much 'cleaner' data, since a person would be less likely to forward junk-mail. In addition to making this useful observation, the writer pointed to a 'filtered' verison of the sent mails in the corpus. Thus, rather than doing the work of filtering through the corpus, the project will progress directly to LDA techniques. Once a rustic model is set up, one can revisit the regural expressions to see if generalisations are possible. \n",
    "\n",
    "** Second link [here](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir, chdir\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import the data into a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "chdir('/home/peter/Downloads/enronsent')\n",
    "for file in listdir():\n",
    "    if file.startswith('enron'):\n",
    "        text = open(file).read()\n",
    "        \n",
    "        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+','',text) # Remove emails\n",
    "        text = re.sub(r'[\\*\\\\\\/\\_\\=\\\"-\\$(...)(~~~)(---)]+', '' ,text) # Remove misc \n",
    "        text = re.sub(r'(\\d+th)|(\\d+)', \"\",text) # Remove arbitrary numbers\n",
    "        text = re.sub(r'\\(.\\)','',text) # Remove possibile multiplicity in words\n",
    "        text = re.sub('\\\\\\'', \"\",text) # NO IDEA HOW TO FILTER THIS OUT!\n",
    "        text = re.sub(r'-----Original Message-----', ' ', text) ###\n",
    "        text = re.sub(r'(will|can|please|can|know|thank)', ' ', text) #######\n",
    "        text = re.sub(r'\\s+',' ',text) # Remove newline and whitespace \n",
    "        text = re.sub(r'omni.* ',' ', text)###3\n",
    "        text = re.sub(r'( . )|( .\\: )',' ', text) # Remove single characters\n",
    "        docs.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TESTING docs DATA\n",
    "print(len(docs))\n",
    "docs[0][1000000:1040000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenization\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # Removing stop words\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # Stemming \n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # stem token\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constructing a document-term matrix\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=7, id2word = dictionary, passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-4f75a6dfeacd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lda_model_array_strict_version.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the LDA model\n",
    "import pickle\n",
    "with open('lda_model_array_strict_version.pkl', 'wb') as f:\n",
    "    pickle.dump(ldamodel, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: carol thank need fax pm enron pleas get agreement\n",
      "Topic 1: thank need pleas ga get let enron deal work\n",
      "Topic 2: thank enron get go need time like pleas work\n",
      "Topic 3: thank enron pleas email vinc work need attach pm\n",
      "Topic 4: thank deal go get let just need im pm\n",
      "Topic 5: power messag thank enron state get california call energi\n",
      "Topic 6: pm cndjohn forneyoudhouodect omniupdatedbi omnicalendarentryid omniorgt omnicalendarentri omniappointmenttyp omnienddatetim\n"
     ]
    }
   ],
   "source": [
    "num_topics = 7\n",
    "num_words = 9\n",
    "\n",
    "List = ldamodel.print_topics(num_topics, num_words)\n",
    "for i in range(0,len(List)):\n",
    "    print('Topic ' + str(i) + ': ' + str(re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '',List[i][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
