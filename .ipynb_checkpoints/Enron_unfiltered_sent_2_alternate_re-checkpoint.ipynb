{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling of the Enron corpus, using LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the \"sent\" directory of each of the 150 employees of Enron. We need to import the data and in turn, clean up the data. Info from [here](https://rforwork.info/2013/11/03/a-rather-nosy-topic-model-analysis-of-the-enron-email-corpus/) and here [here](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) proved to be very useful. Also see http://www.colorado.edu/ics/sites/default/files/attached-files/01-11_0.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed autotime.py. To use it, type:\n",
      "  %load_ext autotime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension(s)as a python packages.\n",
      "  \"as a python packages.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# We use the following magic commands to time the cells in the notebook\n",
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "\n",
    "from os import listdir, chdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to place all the emails of each user into one large list. In order to utalise the LDA algorithm we require there to me multiple documents. The obvious question that arises is whether to consider each email as a seperate document, or to consider the collection of each user's emails as a seperate document. For example:\n",
    "\n",
    "Consider person $A$ has emails $A_1$, $A_2$, $A_3$ and person $B$ has emails $B_1$ and $B_2$. Then we can create a list that is L = [$A_1$, $A_2$, $A_3$, $B_1$, $B_2$] or L = [$A_1A_2A_3$, $B_1B_2$]. For now, all the emails are going to be treated as seperate documents. \n",
    "\n",
    "Once the LDA algorithm has been implemented, we want to be able to list all the documents that fall under a given catagory. \n",
    "\n",
    "We now set up the regular expressions to remove the 'clutter' from the emails.\n",
    "(Note, they are purposefully long to avoid successive searches through large data)\n",
    "\n",
    "An alternate set of regular expressions are also included. These are seperated and thus take longer to iterate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Defining regular expressions \n",
    "\n",
    "re1 = re.compile('(Message-ID(.*?\\n)*X-FileName.*?\\n)|'\n",
    "                 '(To:(.*?\\n)*?Subject.*?\\n)|'\n",
    "                 '(< (Message-ID(.*?\\n)*.*?X-FileName.*?\\n))')\n",
    "re2 = re.compile('<|'\n",
    "                 '>|'\n",
    "                 '(---(.*?\\n)?.*?---)|'\n",
    "                 '(\\*\\*[.*?\\s]\\*\\*)|'\n",
    "                 '(.*?:(\\s|(.*?\\s)|))|'\n",
    "                 '(\\(\\d+\\))|'\n",
    "                 '(\\s.*?\\..*?\\s)|'\n",
    "                 '(\\s.*?\\_.*?\\s)|'\n",
    "                 '(\\s.*?\\-.*?\\s)|'\n",
    "                 '(\\s.*\\/.*?\\s)|'\n",
    "                 '(\\s.*@.*?\\s)|'\n",
    "                 '([\\d\\-\\(\\)\\\\\\/\\#\\=]+(\\s|\\.))|'\n",
    "                 '(\\n.*?\\s)|\\d')\n",
    "re3 = re.compile('\\\\\\'')\n",
    "re4 = re.compile('( . )|\\s+')\n",
    "\n",
    "#Alternate set of regular expressions\n",
    "\n",
    "re0_a = re.compile('>')\n",
    "re1_a = re.compile('(Message-ID(.*?\\n)*X-FileName.*?\\n)|'\n",
    "                 '(To:(.*?\\n)*?Subject.*?\\n)|'\n",
    "                 '(< (Message-ID(.*?\\n)*.*?X-FileName.*?\\n))')\n",
    "re2_a = re.compile('(.+)@(.+)') # Remove emails\n",
    "re3_a = re.compile('\\s(-----)(.*?)(-----)\\s', re.DOTALL)\n",
    "re4_a = re.compile('''\\s(\\*\\*\\*\\*\\*)(.*?)(\\*\\*\\*\\*\\*)\\s''', re.DOTALL)\n",
    "re5_a = re.compile('\\s(_____)(.*?)(_____)\\s', re.DOTALL)\n",
    "re6_a = re.compile('\\n( )*-.*')\n",
    "re7_a = re.compile('\\n( )*\\d.*')\n",
    "re8_a = re.compile('(\\n( )*[\\w]+($|( )*\\n))|(\\n( )*(\\w)+(\\s)+(\\w)+(( )*\\n)|$)|(\\n( )*(\\w)+(\\s)+(\\w)+(\\s)+(\\w)+(( )*\\n)|$)')\n",
    "re9_a = re.compile('.*orwarded.*')\n",
    "re10_a = re.compile('From.*|Sent.*|cc.*|Subject.*|Embedded.*|http.*|\\w+\\.\\w+|.*\\d\\d/\\d\\d/\\d\\d\\d\\d.*')\n",
    "re11_a = re.compile(' [\\d:;,.]+ ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a list of strings - each string being an email (document). Each document is filtered according to the regular expressions above. We also build a dictionary, namely, docs_num_dict that stores for each iteration of a name, the corresponding name and as well as a list of the filtered text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "docs = []\n",
    "docs_num_dict = [] # Stores email sender's name and number\n",
    "\n",
    "chdir('/home/peter/Downloads/enron')\n",
    "# For each user we extract all the emails in their inbox\n",
    "\n",
    "names = [i for i in listdir()]\n",
    "m = 0\n",
    "for name in names:\n",
    "    sent = '/home/peter/Downloads/enron/' + str(name) + '/sent'   \n",
    "    try: \n",
    "        chdir(sent)\n",
    "        d = []\n",
    "        for email in listdir():          \n",
    "            text = open(email,'r').read()\n",
    "            # Regular expressions are used below to remove 'clutter'\n",
    "            text = re.sub(re0_a, ' ', text)\n",
    "            text = re.sub(re1_a, ' ', text)\n",
    "            text = re.sub(re2_a, ' ', text)\n",
    "            text = re.sub(re3_a, ' ', text)\n",
    "            text = re.sub(re4_a, ' ', text)\n",
    "            text = re.sub(re5_a, ' ', text)\n",
    "            text = re.sub(re6_a, ' ', text)\n",
    "            text = re.sub(re7_a, ' ', text)\n",
    "            text = re.sub(re8_a, ' ', text)\n",
    "            text = re.sub(re9_a, ' ', text)\n",
    "            text = re.sub(re10_a, ' ', text)\n",
    "            text = re.sub(re11_a, ' ', text)\n",
    "            docs.append(text)\n",
    "            d.append(text)\n",
    "        docs_num_dict.append((m,[name,d]))\n",
    "        m += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "docs_num_dict = dict(docs_num_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of either a) Stemming or b) Lemmatizing to find word roots. See [here](http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization) for a more detailed explination of the two. Right below, the lemmatizer is implemented. \n",
    "\n",
    "The stemmer generally cuts off prefixes of words according to some set rules. Thus words like 'facilitate' and shortened to 'faci' - this can be confusing and requires that the words are 're-built' before displayed. The lemmatizer also used set rules for words of a certain form, but it has the advantage of comparing words to a dictionary.\n",
    "\n",
    "In general, the lemmatizer will have preference of use. \n",
    "\n",
    "While creating a new 'texts' variable that stores the filtered documents, we also edit the docs_num_dict to update the words according to the tokenize,stop word, lemmatize procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the lemmatizer (consider using this instead of the stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To build the dictionary\n",
    "from collections import defaultdict\n",
    "d = defaultdict(int)\n",
    "\n",
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for i in range(0,len(docs_num_dict.items())):\n",
    "    new_docs_num_dict_1 = []\n",
    "    for doc in docs_num_dict[i][1]:\n",
    "        # Tokenization\n",
    "        raw = doc.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # Removing stop words\n",
    "\n",
    "        # create English stop words list\n",
    "        en_stop = get_stop_words('en')\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "        # Stemming \n",
    "\n",
    "        # Create p_stemmer of class PorterStemmer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # lemmatize token\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "\n",
    "        texts.append(lemmatized_tokens)\n",
    "        new_docs_num_dict_1.append(lemmatized_tokens)\n",
    "\n",
    "        # We now build the dictionary\n",
    "        for word in lemmatized_tokens:\n",
    "            d[word] += 1  \n",
    "    docs_num_dict[i][1] = new_docs_num_dict_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts file as well as the dictinary d (this counts the total number of times a given word is used in the corpus) is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# Save the texts file as texts_raw (will be edited again below)\n",
    "with open('texts_raw.jsn','w') as f:\n",
    "    json.dump(texts,f)\n",
    "f.close()\n",
    "\n",
    "# Save the dictionary d\n",
    "with open('d.jsn','w') as f:\n",
    "    json.dump(d,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# Loading the raw texts file\n",
    "with open('texts_raw.jsn','r') as f:\n",
    "    texts = json.load(f)\n",
    "f.close()\n",
    "    \n",
    "# Loading the dictionary d \n",
    "with open('d.jsn','r') as f:\n",
    "    d = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the dictionary of dictionaries, docs_name_dict. The dictinary associates to the names of each employee, a dictionary that stores all the words used by the given person, as well as the number of times they used each of these words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "docs_name_dict = []\n",
    "\n",
    "for i in range(0,len(docs_num_dict.items())):\n",
    "    temp_dict = defaultdict(int)\n",
    "    for j in docs_num_dict[i][1]:\n",
    "        for k in j:\n",
    "            temp_dict[k] += 1\n",
    "    # Append the temporary dictionary to docs_name_dict\n",
    "    docs_name_dict.append((docs_num_dict[i][0],temp_dict)) \n",
    "docs_name_dict = dict(docs_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to remove the words from our documents that cause clutter. We will remove all the words that appear in more than 20% of documents as well as removing all the words that occur in less than 4 of the documents. We have a dictionary that counts the number of times a word in present across all the $\\pm57000$ documents. \n",
    "\n",
    "To further enhance the quality of the text we analyse, the loops below remove all words of length 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_docs = len(texts)\n",
    "temp_texts = texts\n",
    "texts= []\n",
    "upper_lim = int(0.20*num_docs)\n",
    "\n",
    "for doc in temp_texts:\n",
    "    temp_doc = []\n",
    "    for word in doc:\n",
    "        # If the word is in the required interval, we add it to a NEW texts variable\n",
    "        if 4 < d[word] < upper_lim and len(word) > 2:\n",
    "            temp_doc.append(word)\n",
    "        # If the word is not in the required interval, \n",
    "        # we lower the index of the word in the docs_name_dict dictinoary\n",
    "        else:\n",
    "            for group in docs_name_dict.items():\n",
    "                person = group[0]\n",
    "                if word in docs_name_dict[person]:\n",
    "                    if docs_name_dict[person][word] > 1:\n",
    "                        docs_name_dict[person][word] -= 1\n",
    "                    else:\n",
    "                        del docs_name_dict[person][word]\n",
    "    texts.append(temp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to save the refined texts file and the dictionary, docs_name_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# We save the new 'refined' texts file\n",
    "with open('texts.jsn','w') as f:\n",
    "    json.dump(texts,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# We save the docs_name_dict global person, word-count dictionary\n",
    "pickle.dump( docs_name_dict , open( \"docs_name_dict.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# Loading the texts file\n",
    "with open('texts.jsn', 'r') as f:\n",
    "    texts = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# Loading the docs_name_dict dicitonary\n",
    "docs_name_dict = pickle.load( open( \"docs_name_dict.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the document term matrix whereafter the fairly lengthy process of constructing the model takes place. Thus far the model seems be linear. With a single pass, the model takes just upward of a minute to execute, whereas for 5 passes, the model takes roughly 5.5 minutes.\n",
    "\n",
    "The model was run for 350 passes and took 316 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constructing a document-term matrix\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save both the LDA data as well as the results. We can reanalyse later. See the folder called LDAdata.\n",
    "\n",
    "To load the files again:\n",
    "\n",
    "ldamodel = models.LdaModel.load('ldamodel.model') and dictionary = corpora.Dictionary.load('dictionary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Saving the dictionary\n",
    "dictionary.save('dictionary')\n",
    "\n",
    "# Saving the corpus    \n",
    "with open('corpus.jsn','w') as f:\n",
    "    json.dump(corpus,f)    \n",
    "f.close()\n",
    "\n",
    "# Saving the ldamodel\n",
    "ldamodel.save('ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load dictionary\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load ldamodel\n",
    "ldamodel = models.LdaModel.load('ldamodel') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load corpus\n",
    "with open('corpus.jsn','r') as f:\n",
    "    corpus = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the words for each of the given topics. It must be noted, that even though considerable emphasis has been placed on the construction of the regular expressions, 'junk-text' may be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "num_words = 10\n",
    "\n",
    "List = ldamodel.print_topics(num_topics, num_words)\n",
    "Topic_words =[]\n",
    "for i in range(0,len(List)):\n",
    "    word_list = re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '',List[i][1])\n",
    "    temp = [word for word in word_list.split()]\n",
    "    Topic_words.append(temp)\n",
    "    print('Topic ' + str(i) + ': ' + '\\n' + str(word_list))\n",
    "    print('\\n' + '-'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of words created above is saved below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Saving the list of words\n",
    "with open('topic_words.jsn','w') as f:\n",
    "    json.dump(Topic_words,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(Topic_words)):\n",
    "    temp = Topic_words[i]\n",
    "    sort_key = lambda s: (-len(s), s)\n",
    "    temp.sort(key = sort_key)\n",
    "    print(temp)\n",
    "    Topic_words[i] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to export the list of words in a csv file such that we can use the data in out D3 visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "with open('topic_words.jsn','r') as f:\n",
    "    Topic_words = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to visualise the data above by using the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/index.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_visualise = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(lda_visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We use the colour pallate called Tableau_20 that contains 20 different colours. We assign these to seperate topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from palettable.tableau import Tableau_20\n",
    "\n",
    "topic_colour_gen = []\n",
    "for i in range(0,num_topics):\n",
    "    topic_colour_gen.append((i, Tableau_20.hex_colors[i]))\n",
    "    \n",
    "topic_colours = dict(topic_colour_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below runs through a document of the user's choice and matches topic words within the document, highlighting them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "doc = ''\n",
    "\n",
    "def match_words(word):\n",
    "    word_edit = word.lower()\n",
    "    try:\n",
    "        word_edit = tokenizer.tokenize(word_edit)[0]\n",
    "    except:\n",
    "        pass\n",
    "    return wordnet_lemmatizer.lemmatize(word_edit)\n",
    "    \n",
    "def build_html_colour(word, topic):\n",
    "    #return \" <font color=\" + str(topic_colours[topic]) + \"'>\" + word + \"</font> \"\n",
    "    return ' <span style=\"background-color: ' + str(topic_colours[topic])  +'\">' + word + '</span>'\n",
    "\n",
    "def read_doc(doc):\n",
    "    chdir('/home/peter/Topic_Modelling/LDA/text_files')\n",
    "    doc = open(str(doc),'r').read()\n",
    "    \n",
    "    # Variables so recalculation is not necessary\n",
    "    doc_split = doc.split()\n",
    "    \n",
    "    # Build dictionary of topic's distribution for a given document\n",
    "    num_topics_weight = 0\n",
    "    Topics = defaultdict(int)\n",
    "    for word in doc_split:\n",
    "        word_edit = match_words(word)\n",
    "        try:\n",
    "            word_topics = ldamodel.get_term_topics(word_edit)\n",
    "            if word_topics:\n",
    "                for topic in word_topics:\n",
    "                    Topics[topic[0]] += topic[1]\n",
    "                    num_topics_weight += topic[1]            \n",
    "        except:\n",
    "            pass\n",
    "    # Find topic info\n",
    "    # Append Topic, number of words in document from given topic and doc percentage of topic\n",
    "    Topic_info = []\n",
    "    for topic in Topics:\n",
    "        Topic_info.append([topic, Topics[topic], round((Topics[topic]/num_topics_weight)*100)]) \n",
    "    \n",
    "    # Topic info for three most prevalent topics for a given document\n",
    "    Topic_info_top3 = []\n",
    "    Topic_info_copy = []\n",
    "    for i in Topic_info:\n",
    "        Topic_info_copy.append(i)\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        max = Topic_info_copy[0]\n",
    "        for topic in Topic_info_copy:\n",
    "            if topic[2] > max[2]:\n",
    "                max = topic\n",
    "        Topic_info_top3.append(max)\n",
    "        Topic_info_copy.remove(max)\n",
    "        \n",
    "    \n",
    "    # Format the document according to topics\n",
    "    for word in doc_split:\n",
    "        word_edit = match_words(word)\n",
    "        try:\n",
    "            topic = ldamodel.get_term_topics(word_edit)[0][0]\n",
    "            if (topic == Topic_info_top3[0][0]) or (topic == Topic_info_top3[1][0]) or (topic == Topic_info_top3[2][0]):\n",
    "                doc = doc.replace( ' ' + word + '', build_html_colour(word,topic))\n",
    "                #doc = doc.replace( '' + word + ' ', build_html_colour(word,topic))\n",
    "        except:\n",
    "            pass\n",
    "    doc = re.sub(r'\\n','<br>',doc)\n",
    "    \n",
    "    Output = []\n",
    "    for item in Topic_info_top3:\n",
    "        colour = build_html_colour('Topic ' + str(item[0]), item[0])\n",
    "        topic_info = colour + ': ' + str(item[2]) + '% ' + str(Topic_words[item[0]])\n",
    "        Output.append(topic_info)\n",
    "    return Output, doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is used to add colour to the printed text. See [here](https://jakevdp.github.io/blog/2013/06/01/ipython-notebook-javascript-python-communication/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example from http://jakevdp.github.io/blog/2013/06/01/ipython-notebook-javascript-python-communication/ adapted for IPython 2.0\n",
    "\n",
    "#Input the document we want to read\n",
    "\n",
    "doc = 'dickson-s_3.'\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "input_form = \"\"\"\n",
    "<div style=\"background-color:white; border:solid black; width:1100px; padding:20px;\">\n",
    "<p>\"\"\"+read_doc(doc)[0][0]+\"\"\"</p>\n",
    "<p>\"\"\"+read_doc(doc)[0][1]+\"\"\"</p>\n",
    "<p>\"\"\"+read_doc(doc)[0][2]+\"\"\"</p>\n",
    "<p>\"\"\"+read_doc(doc)[1]+\"\"\"</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "HTML(input_form) # + javascript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now also have a method to see which topics are prevalent for a given person.\n",
    "\n",
    "Below, we create two functions, namely, get_person_topics and get_topic_persons.\n",
    "\n",
    "get_person_topics takes in a specific person as a string and returns a dictionary with a ratio value (out of 1) for each of the 20 topics. This indicates the prevalance of each of the topics as a percentage for a given person.\n",
    "\n",
    "get_topic_persons takes in a topic as an integer and returns a dictionary with a ratio value (out of 1) for all the employees. This indicates which employees fall under a specific topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_person_topics(person):\n",
    "    person_topics = defaultdict(int)\n",
    "    total = 0\n",
    "    for word in docs_name_dict[person]:\n",
    "        try:\n",
    "            term_topics = ldamodel.get_term_topics(word)\n",
    "            if term_topics:\n",
    "                for topic_tuple in term_topics:\n",
    "                    person_topics[topic_tuple[0]] += topic_tuple[1]\n",
    "                    total += topic_tuple[1]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    #scale the values\n",
    "    for person in person_topics:\n",
    "        person_topics[person] = person_topics[person]/total\n",
    "    return person_topics\n",
    "\n",
    "def get_topic_persons(topic):\n",
    "    specific_topic_persons = defaultdict(int)\n",
    "    \n",
    "    total = 0\n",
    "    for person in docs_name_dict:\n",
    "        person_topics = get_person_topics(person)\n",
    "        person_value = person_topics[topic]\n",
    "        specific_topic_persons[person] += person_value\n",
    "        total += person_value\n",
    "    \n",
    "    \n",
    "    #Scale the numbers in the dictionary to a percentage\n",
    "    for person in docs_name_dict:\n",
    "        specific_topic_persons[person] = specific_topic_persons[person]/total\n",
    "        \n",
    "    return specific_topic_persons\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see which person falls under a given topic the 'most' as well as which topic falls under a given person the 'most'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding top person for a given topic\n",
    "\n",
    "topic_person = get_topic_persons(10)\n",
    "maximum_person = max(topic_person.keys(), key=(lambda key: topic_person[key]))\n",
    "print(maximum_person, '{0:.2%}'.format(topic_person[maximum_person]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding top topic for a given person\n",
    "\n",
    "person_topic = get_person_topics('allen-p')\n",
    "maximum_topic = max(person_topic.keys(), key=(lambda key: person_topic[key]))\n",
    "print(maximum_topic, '{0:.2%}'.format(person_topic[maximum_topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now make use of matplotlib to plot the above data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tot_words_person(person):\n",
    "    n = 0\n",
    "    for word in docs_name_dict[person]:\n",
    "        n += docs_name_dict[person][word]\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a datastructure to export as a csv.\n",
    "The data fields are,\n",
    "\n",
    "$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\\hline\n",
    "\\text{Person Name} & \\text{id} & \\text{Top Topic} & \\text{Top Topic} & \\text{Second Topic} & \\text{Second Topic} &\\ldots  \\\\\\hline\n",
    "\\text{Dickson, S} & \\text{dickson-s} & \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ \\hline\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = []\n",
    "list_of_names = []\n",
    "list_of_names_dup = []\n",
    "for name in docs_name_dict:\n",
    "    list_of_names.append(name.capitalize().replace('-',', '))\n",
    "    list_of_names_dup.append(name)\n",
    "list_of_names.sort()\n",
    "list_of_names_dup.sort()\n",
    "\n",
    "for i in range(0,len(list_of_names)):\n",
    "    name = list_of_names[i][0:-1]\n",
    "    first_name = list_of_names[i][-1].capitalize()\n",
    "    list_of_names[i] = name + first_name\n",
    "    Data.append([name+first_name,list_of_names_dup[i],get_tot_words_person(list_of_names_dup[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in Data:\n",
    "    name = data[1]\n",
    "    person_topics = get_person_topics(name)\n",
    "    person_topics = [(v, k) for k, v in person_topics.items()]\n",
    "    person_topics.sort()\n",
    "    person_topics.reverse()\n",
    "    for tuples in person_topics:\n",
    "        data.append(tuples[1])\n",
    "        data.append(tuples[0])\n",
    "    L = range(0,20)\n",
    "    for num in L:\n",
    "        if num not in data:\n",
    "            data.append(num)\n",
    "            data.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data = [['Employee', 'id', 'tot_words', 'A', 'Ap', 'B', 'Bp', 'C', 'Cp'\n",
    "         , 'D', 'Dp', 'E', 'Ep', 'F', 'Fp', 'G', 'Gp', 'H', 'Hp',\n",
    "         'I', 'Ip', 'J', 'Jp', 'K', 'Kp', 'L', 'Lp', 'M', 'Mp', 'N', \n",
    "         'Np', 'O', 'Op', 'P', 'Pp', 'Q', 'Qp', 'R', 'Rp', 'S', 'Sp', 'T', 'Tp']] + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"bubbles_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(Data)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
