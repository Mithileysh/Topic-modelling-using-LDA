{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling of the Enron corpus, using LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the \"sent\" directory of each of the 150 employees of Enron. We need to import the data and in turn, clean up the data. Info from [here](https://rforwork.info/2013/11/03/a-rather-nosy-topic-model-analysis-of-the-enron-email-corpus/) and here [here](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) proved to be very useful. Also see http://www.colorado.edu/ics/sites/default/files/attached-files/01-11_0.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed autotime.py. To use it, type:\n",
      "  %load_ext autotime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension(s)as a python packages.\n",
      "  \"as a python packages.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# We use the following magic commands to time the cells in the notebook\n",
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "\n",
    "from os import listdir, chdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to place all the emails of each user into one large list. In order to utalise the LDA algorithm we require there to me multiple documents. The obvious question that arises is whether to consider each email as a seperate document, or to consider the collection of each user's emails as a seperate document. For example:\n",
    "\n",
    "Consider person $A$ has emails $A_1$, $A_2$, $A_3$ and person $B$ has emails $B_1$ and $B_2$. Then we can create a list that is L = [$A_1$, $A_2$, $A_3$, $B_1$, $B_2$] or L = [$A_1A_2A_3$, $B_1B_2$]. For now, all the emails are going to be treated as seperate documents. \n",
    "\n",
    "Once the LDA algorithm has been implemented, we want to be able to list all the documents that fall under a given catagory. \n",
    "\n",
    "We now set up the regular expressions to remove the 'clutter' from the emails.\n",
    "(Note, they are purposefully long to avoid successive searches through large data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.2 ms\n"
     ]
    }
   ],
   "source": [
    "# Defining regular expressions \n",
    "\n",
    "re1 = re.compile('(Message-ID(.*?\\n)*X-FileName.*?\\n)|'\n",
    "                 '(To:(.*?\\n)*?Subject.*?\\n)|'\n",
    "                 '(< (Message-ID(.*?\\n)*.*?X-FileName.*?\\n))')\n",
    "re2 = re.compile('<|'\n",
    "                 '>|'\n",
    "                 '(---(.*?\\n)?.*?---)|'\n",
    "                 '(\\*\\*[.*?\\s]\\*\\*)|'\n",
    "                 '(.*?:(\\s|(.*?\\s)|))|'\n",
    "                 '(\\(\\d+\\))|'\n",
    "                 '(\\s.*?\\..*?\\s)|'\n",
    "                 '(\\s.*?\\_.*?\\s)|'\n",
    "                 '(\\s.*?\\-.*?\\s)|'\n",
    "                 '(\\s.*\\/.*?\\s)|'\n",
    "                 '(\\s.*@.*?\\s)|'\n",
    "                 '([\\d\\-\\(\\)\\\\\\/\\#\\=]+(\\s|\\.))|'\n",
    "                 '(\\n.*?\\s)|\\d')\n",
    "re3 = re.compile('\\\\\\'')\n",
    "re4 = re.compile('( . )|\\s+')\n",
    "#re5 = re.compile('( \\S{1,3} )|( com )|( can )') # Some problem characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the basic document, filtering accroding to our regular expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "chdir('/home/peter/Downloads/enron')\n",
    "# For each user we extract all the emails in their inbox\n",
    "\n",
    "names = [i for i in listdir()]\n",
    "for name in names:\n",
    "    sent = '/home/peter/Downloads/enron/' + str(name) + '/sent'   \n",
    "    try: \n",
    "        chdir(sent)     \n",
    "        for email in listdir():\n",
    "            text = open(email,'r').read()\n",
    "            # Regular expressions are used below to remove 'clutter'\n",
    "            text = re.sub(re1,' ',text)\n",
    "            text = re.sub(re2,' ',text)\n",
    "            text = re.sub(re3,'',text)\n",
    "            text = re.sub(re4,' ',text)\n",
    "            #text = re.sub(re5,' ',text)\n",
    "            docs.append(text)\n",
    "            \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of either a) Stemming or b) Lemmatizing to find word roots. See [here](http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization) for a more detailed explination of the two. Right below, the stemmer is implemented, while two cells below, the lemmatizer is implemented. Make sure to choose which one to use before proceeding to Constructing the document-term matrix.\n",
    "\n",
    "The stemmer generally cuts off prefixes of words according to some set rules. Thus words like 'facilitate' and shortened to 'faci' - this can be confusing and requires that the words are 're-built' before displayed. The lemmatizer also used set rules for words of a certain form, but it has the advantage of comparing words to a dictionary.\n",
    "\n",
    "In general, the lemmatizer will have preference of use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To build the dictionary\n",
    "from collections import defaultdict\n",
    "d = []\n",
    "\n",
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenization\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # Removing stop words\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # Stemming \n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # stem token\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "    # We now build the dictionary\n",
    "    temp_d = defaultdict(int)\n",
    "    for word in stemmed_tokens:\n",
    "        temp_d[word] += 1\n",
    "    d.append(temp_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the lemmatizer (consider using this instead of the stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "# To build the dictionary\n",
    "from collections import defaultdict\n",
    "d = defaultdict(int)\n",
    "\n",
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenization\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # Removing stop words\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # Stemming \n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # stem token\n",
    "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(lemmatized_tokens)\n",
    "    \n",
    "    # We now build the dictionary\n",
    "    for word in lemmatized_tokens:\n",
    "        d[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "# Saving the LDA model to a JSON file\n",
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "with open('texts_raw.jsn','w') as f:\n",
    "    json.dump(texts,f)\n",
    "    \n",
    "with open('d.jsn','w') as f:\n",
    "    json.dump(d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 583 ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "# Loading the LDA model\n",
    "with open('texts_raw.jsn','r') as f:\n",
    "    texts = json.load(f)\n",
    "    \n",
    "# Loading the python dictionary (not to be confused with other dictionary)\n",
    "with open('d.jsn','r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to remove the words from our documents that cause clutter. We will remove all the words that appear in more than 20% of documents as well as removing all the words that occur in less than 4 of the documents. We have a dictionary that counts the number of times a word in present across all the $\\pm57000$ documents. \n",
    "\n",
    "To further enhance the quality of the text we analyse, the loops below remove all words of length 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "num_docs = len(texts)\n",
    "temp_texts = texts\n",
    "texts= []\n",
    "upper_lim = int(0.20*num_docs)\n",
    "\n",
    "for doc in temp_texts:\n",
    "    temp_doc = []\n",
    "    for word in doc:\n",
    "        if 4 < d[word] < upper_lim and len(word) > 2:\n",
    "            temp_doc.append(word)\n",
    "    texts.append(temp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# We save the new 'refined' texts file\n",
    "with open('texts.jsn','w') as f:\n",
    "    json.dump(temp_texts,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 414 ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "chdir('/home/peter/Topic_Modelling/LDA/')\n",
    "\n",
    "# Loading the texts file\n",
    "with open('texts.jsn', 'r') as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the document term matrix whereafter the fairly lengthy process of constructing the model takes place. Thus far the model seems be linear. With a single pass, the model takes just upward of a minute to execute, whereas for 5 passes, the model takes roughly 5.5 minutes.\n",
    "\n",
    "The model was run for 350 passes and took 316 minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.35 s\n"
     ]
    }
   ],
   "source": [
    "# Constructing a document-term matrix\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save both the LDA data as well as the results. We can reanalyse later. See the folder called LDAdata.\n",
    "\n",
    "To load the files again:\n",
    "\n",
    "ldamodel = models.LdaModel.load('ldamodel.model') and dictionary = corpora.Dictionary.load('dictionary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Saving the dictionary\n",
    "dictionary.save('dictionary')\n",
    "\n",
    "# Saving the corpus    \n",
    "with open('corpus.jsn','w') as f:\n",
    "    json.dump(corpus,f)    \n",
    "\n",
    "# Saving the ldamodel\n",
    "ldamodel.save('ldamodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load dictionary\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 83.3 ms\n"
     ]
    }
   ],
   "source": [
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load ldamodel\n",
    "ldamodel = models.LdaModel.load('ldamodel') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Load corpus\n",
    "with open('corpus.jsn','r') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print the words for each of the given topics. It must be noted, that even though considerable emphasis has been placed on the construction of the regular expressions, 'junk-text' may be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "john ect future member broker brent click nymex board jason\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 1: \n",
      "contract party agreement language may transaction issue term credit payment\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 2: \n",
      "power california state energy market said utility price electricity cost\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 3: \n",
      "just get think going one dont day see good time\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 4: \n",
      "city new university houston school student producer san class administration\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 5: \n",
      "information need also project access employee process provide like issue\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 6: \n",
      "agreement attached draft copy document master need change letter form\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 7: \n",
      "fax texas street smith sara houston shackleton phone legal perlingiere\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 8: \n",
      "tax sherri corp court loan land story property judge meter\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 9: \n",
      "gas company energy trading power natural product trade financial pipeline\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 10: \n",
      "vince forwarded love god game sound year man one play\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 11: \n",
      "meeting conference call week friday next thursday time monday schedule\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 12: \n",
      "chris gas ben book daily report volume thanks forwarded need\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 13: \n",
      "business mark group risk management market new service global trading\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 14: \n",
      "deal forwarded thanks delainey desk contract tom mike eol zone\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 15: \n",
      "price shall day option per month date value rate period\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 16: \n",
      "internet kate investor software computer www buy auction world article\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 17: \n",
      "jeff debra john bill richard robert kay dasovich david bob\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 18: \n",
      "know let get jeff need want like thanks call think\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 19: \n",
      "message intended information email communication may received use recipient error\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "time: 42.7 ms\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "num_words = 10\n",
    "\n",
    "List = ldamodel.print_topics(num_topics, num_words)\n",
    "Topic_words =[]\n",
    "for i in range(0,len(List)):\n",
    "    word_list = re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '',List[i][1])\n",
    "    temp = [word for word in word_list.split()]\n",
    "    Topic_words.append(temp)\n",
    "    print('Topic ' + str(i) + ': ' + '\\n' + str(word_list))\n",
    "    print('\\n' + '-'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "# Saving the list of words\n",
    "with open('topic_words.jsn','w') as f:\n",
    "    json.dump(Topic_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.44 ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "chdir('/home/peter/Topic_Modelling/LDA/LDAdata_results')\n",
    "\n",
    "with open('topic_words.jsn','r') as f:\n",
    "    Topic_words = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to visualise the data above by using the [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/index.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 19749 is out of bounds for axis 1 with size 19749",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-03488d769f05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda_visualise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_visualise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda3/lib/python3.5/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda3/lib/python3.5/site-packages/pyLDAvis/gensim.py\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m       \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m       \u001b[0mdoc_topic_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/peter/anaconda3/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 19749 is out of bounds for axis 1 with size 19749"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_visualise = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(lda_visualise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now consider, for a given document, the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Set topic colours (Assigned randomly)\n",
    "import random\n",
    "\n",
    "topic_colour_gen = []\n",
    "\n",
    "for i in range(0,num_topics):\n",
    "    r = lambda: random.randint(0,255)\n",
    "    topic_colour_gen.append((i,'#%02X%02X%02X' % (r(),r(),r())))\n",
    "    \n",
    "topic_colours = dict(topic_colour_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below runs through a document of the user's choice and matches topic words within the document, highlighting them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.1 ms\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "doc = ''\n",
    "\n",
    "def read_doc(doc):\n",
    "    chdir('/home/peter/Topic_Modelling/LDA')\n",
    "    doc = open(str(doc),'r').read()\n",
    "    Topics = defaultdict(int)\n",
    "    for word in doc.split():\n",
    "        word_edit = word.lower()\n",
    "        try:\n",
    "            word_edit = tokenizer.tokenize(word_edit)[0]\n",
    "        except:\n",
    "            pass\n",
    "        word_edit = wordnet_lemmatizer.lemmatize(word_edit)\n",
    "        try:\n",
    "            topic = ldamodel.get_term_topics(word_edit)[0][0] \n",
    "            Topics[topic] += 1\n",
    "            doc = doc.replace( ' ' + word + ' ', \" <font color=\" + str(topic_colours[topic]) + \"'>\" + word + \"</font> \")      \n",
    "        except:\n",
    "            pass\n",
    "    doc = re.sub(r'\\n','<br>',doc)\n",
    "    \n",
    "    Topic_info = []\n",
    "    num_topics = 0\n",
    "    for topic in Topics:\n",
    "        num_topics += Topics[topic]\n",
    "        Topic_info.append([topic,Topics[topic]]) #Append Topic, number of words in document form topic and topic colour\n",
    "    for item in Topic_info:\n",
    "        item.append(round(item[1]/num_topics*100))\n",
    "    for item in Topic_info:\n",
    "        print('Topic ' + str(item[0]) + ': ' + str(item[2]) + '% ' + str(Topic_words[item[0]] ))\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 3: 2% ['just', 'get', 'think', 'going', 'one', 'dont', 'day', 'see', 'good', 'time']\n",
      "Topic 1: 15% ['contract', 'party', 'agreement', 'language', 'may', 'transaction', 'issue', 'term', 'credit', 'payment']\n",
      "Topic 18: 7% ['know', 'let', 'get', 'jeff', 'need', 'want', 'like', 'thanks', 'call', 'think']\n",
      "Topic 19: 2% ['message', 'intended', 'information', 'email', 'communication', 'may', 'received', 'use', 'recipient', 'error']\n",
      "Topic 5: 5% ['information', 'need', 'also', 'project', 'access', 'employee', 'process', 'provide', 'like', 'issue']\n",
      "Topic 6: 41% ['agreement', 'attached', 'draft', 'copy', 'document', 'master', 'need', 'change', 'letter', 'form']\n",
      "Topic 9: 5% ['gas', 'company', 'energy', 'trading', 'power', 'natural', 'product', 'trade', 'financial', 'pipeline']\n",
      "Topic 12: 2% ['chris', 'gas', 'ben', 'book', 'daily', 'report', 'volume', 'thanks', 'forwarded', 'need']\n",
      "Topic 13: 7% ['business', 'mark', 'group', 'risk', 'management', 'market', 'new', 'service', 'global', 'trading']\n",
      "Topic 14: 10% ['deal', 'forwarded', 'thanks', 'delainey', 'desk', 'contract', 'tom', 'mike', 'eol', 'zone']\n",
      "Topic 15: 2% ['price', 'shall', 'day', 'option', 'per', 'month', 'date', 'value', 'rate', 'period']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"background-color:white; border:solid black; width:1100px; padding:20px;\">\n",
       "<p>Message-ID: <26722187.1075842275270.JavaMail.evans@thyme><br>Date: Tue, 12 Dec 2000 05:35:00 -0800 (PST)<br>From: dan.hyvl@enron.com<br>To: mcesario@ormet.com<br>Subject: Re: Ormet Documents<br>Cc: craig.breslau@enron.com, dan.j.hyvl@enron.com, <br>\tmkleinginna.wheelingpo.wheeling_domain@ormet.com, <br>\tttemple.wheelingpo.wheeling_domain@ormet.com, <br>\tjason.williams@enron.com<br>Mime-Version: 1.0<br>Content-Type: text/plain; charset=us-ascii<br>Content-Transfer-Encoding: 7bit<br>Bcc: craig.breslau@enron.com, dan.j.hyvl@enron.com, <br>\tmkleinginna.wheelingpo.wheeling_domain@ormet.com, <br>\tttemple.wheelingpo.wheeling_domain@ormet.com, <br>\tjason.williams@enron.com<br>X-From: Dan J Hyvl<br>X-To: <font color=#E7B013'>\"Mike</font> Cesario\" <MCesario@ormet.com>@ENRON<br>X-cc: Craig.Breslau@enron.com, Dan.J.Hyvl@enron.com, <font color=#E3344B'>\"Mark</font> Kleinginna\" <MKleinginna.WheelingPO.Wheeling_Domain@ormet.com>, \"Tommy Temple\" <TTemple.WheelingPO.Wheeling_Domain@ormet.com>, Jason R Williams<br>X-bcc: <br>X-Folder: \\Dan_Hyvl_Dec2000_June2001\\Notes Folders\\Sent<br>X-Origin: HYVL-D<br>X-FileName: dhyvl.nsf<br><br>Mike,<br> I have checked with the credit <font color=#E3344B'>group</font> and the suggested <font color=#EF7201'>changes</font> to the <br>confidentiality <font color=#6A5B23'>agreement</font> and to Exhibit \"C\" and \"D\" are acceptable.  They <br>did point out a typo in the collateral threshold provision.  It should read <br>$1,500,000 instead of $1,5000,000.  Please accept the redlined <font color=#EF7201'>changes</font> to the <br>documents and <font color=#DBFFEE'>make</font> the above correction and the <font color=#EF7201'>documents</font> should be ready for <br>you to start the execution process.  I am informed that credit will not <font color=#DBFFEE'>make</font> <br>any <font color=#EF7201'>changes</font> of the $1,500,000 collateral threshold amount until they have had <br>an opportunity to <font color=#EF7201'>review</font> the Ormet <font color=#A8E08F'>financial</font> <font color=#B98805'>information.</font>  Thereafter we can <br>work on an amendment if they feel that the <font color=#A8E08F'>financial</font> <font color=#B98805'>information</font> warrants <br>making a change.<br><br><br><br>\t\"Mike Cesario\" <MCesario@ormet.com><br>\t12/12/2000 10:53 AM<br>\t\t <br>\t\t To: <Craig.Breslau@enron.com>, <Dan.J.Hyvl@enron.com><br>\t\t cc: <font color=#E3344B'>\"Mark</font> Kleinginna\" <MKleinginna.WheelingPO.Wheeling_Domain@ormet.com>, <br>\"Tommy Temple\" <TTemple.WheelingPO.Wheeling_Domain@ormet.com><br>\t\t Subject: Ormet Documents<br><br><br>Attached is a marked-up <font color=#EF7201'>draft</font> of the Enron standard confidentiality <br>agreement.  Please <font color=#EF7201'>review</font> it and <font color=#BFE209'>get</font> back to me with your comments.<br><br>Additionally, I have <font color=#EF7201'>attached</font> a <font color=#EF7201'>copy</font> of the Firm Purchase/Sale <font color=#6A5B23'>Agreement</font> that <br>makes Ormet's <font color=#EF7201'>changes</font> to the Guarantee <font color=#6A5B23'>Agreement</font> on Exhibit D consistent on <br>the Exhibit C Guarantee Agreement.<br><br>Pending your <font color=#EF7201'>comments</font> on the Confidentialty <font color=#6A5B23'>Agreement,</font> the only item <br>outstanding is the $1.5 million collateral threshhold.<br><br>I look forward to hearing from you as soon as possible.<br><br>Thanks,<br>Mike<br><br> - Enron_CA.doc<br> - Enron_Agreement.doc<br></p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "# Example from http://jakevdp.github.io/blog/2013/06/01/ipython-notebook-javascript-python-communication/ adapted for IPython 2.0\n",
    "# Add an input form similar to what we saw above\n",
    "\n",
    "# Document: <input type=\"text\" id=\"doc_input\" size=\"5\" height=\"2\" value=\"\"><br>\n",
    "# <button onclick=\"exec_code()\">Execute</button>\n",
    "\n",
    "#Input the document we want to read\n",
    "doc = '135.'\n",
    "\n",
    "from IPython.display import HTML\n",
    "from math import pi, sin\n",
    "\n",
    "input_form = \"\"\"\n",
    "<div style=\"background-color:white; border:solid black; width:1100px; padding:20px;\">\n",
    "<p>\"\"\"+read_doc(doc)+\"\"\"</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# javascript = \"\"\"\n",
    "# <script type=\"text/Javascript\">\n",
    "#     function exec_code(){\n",
    "#         var var_name = document.getElementById('doc_input').value;\n",
    "#         var command = \"doc\" + \" = \" + \"read_doc\" + \"(\" + \"'\" + var_name + \"'\" + \")\";\n",
    "#         console.log(\"Executing Command: \" + command);      \n",
    "#         var kernel = IPython.notebook.kernel;\n",
    "#         text_to_print = kernel.execute(command);\n",
    "#     }\n",
    "# </script>\n",
    "# \"\"\"\n",
    " \n",
    "HTML(input_form) # + javascript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
