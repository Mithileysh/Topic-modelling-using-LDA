{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the \"sent\" directory of each of the 150 employees of Enron. We need to import the data and in turn, clean up the data. Info from [here](https://rforwork.info/2013/11/03/a-rather-nosy-topic-model-analysis-of-the-enron-email-corpus/) and here [here](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) proved to be very useful. Also see http://www.colorado.edu/ics/sites/default/files/attached-files/01-11_0.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed autotime.py. To use it, type:\n",
      "  %load_ext autotime\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 1.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.5/site-packages/IPython/core/magics/extension.py:47: UserWarning: %install_ext` is deprecated, please distribute your extension(s)as a python packages.\n",
      "  \"as a python packages.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "from os import listdir, chdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to place all the emails of each user into one large list. In order to utalise the LDA algorithm we require there to me multiple documents. The obvious question that arises is whether to consider each email as a seperate document, or to consider the collection of each user's emails as a seperate document. For example:\n",
    "\n",
    "Consider person $A$ has emails $A_1$, $A_2$, $A_3$ and person $B$ has emails $B_1$ and $B_2$. Then we can create a list that is L = [$A_1$, $A_2$, $A_3$, $B_1$, $B_2$] or L = [$A_1A_2A_3$, $B_1B_2$]. For now, all the emails are going to be treated as seperate documents. \n",
    "\n",
    "Once the LDA algorithm has been implemented, we want to be able to list all the documents that fall under a given catagory. \n",
    "\n",
    "We now set up the regular expressions to remove the 'clutter' from the emails.\n",
    "(Note, they are purposefully long to avoid successive searches through large data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "# Defining regular expressions \n",
    "\n",
    "re1 = re.compile('(Message-ID(.*?\\n)*X-FileName.*?\\n)|'\n",
    "                 '(To:(.*?\\n)*?Subject.*?\\n)|'\n",
    "                 '(< (Message-ID(.*?\\n)*.*?X-FileName.*?\\n))')\n",
    "re2 = re.compile('<|'\n",
    "                 '>|'\n",
    "                 '(---(.*?\\n)?.*?---)|'\n",
    "                 '(\\*\\*[.*?\\s]\\*\\*)|'\n",
    "                 '(.*?:(\\s|(.*?\\s)|))|'\n",
    "                 '(\\(\\d+\\))|'\n",
    "                 '(\\s.*?\\..*?\\s)|'\n",
    "                 '(\\s.*?\\_.*?\\s)|'\n",
    "                 '(\\s.*?\\-.*?\\s)|'\n",
    "                 '(\\s.*\\/.*?\\s)|'\n",
    "                 '(\\s.*@.*?\\s)|'\n",
    "                 '([\\d\\-\\(\\)\\\\\\/\\#\\=]+(\\s|\\.))|'\n",
    "                 '(\\n.*?\\s)|\\d')\n",
    "re3 = re.compile('\\\\\\'')\n",
    "re4 = re.compile('( . )|\\s+')\n",
    "re5 = re.compile('( \\S{1,3} )|( com )|( can )') # Some problem characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "chdir('/home/peter/Downloads/enron')\n",
    "# For each user we extract all the emails in their inbox\n",
    "\n",
    "names = [i for i in listdir()]\n",
    "for name in names:\n",
    "    sent = '/home/peter/Downloads/enron/' + str(name) + '/sent'   \n",
    "    try: \n",
    "        chdir(sent)     \n",
    "        for email in listdir():\n",
    "            text = open(email,'r').read()\n",
    "            # Regular expressions are used below to remove 'clutter'\n",
    "            text = re.sub(re1,' ',text)\n",
    "            text = re.sub(re2,' ',text)\n",
    "            text = re.sub(re3,'',text)\n",
    "            text = re.sub(re4,' ',text)\n",
    "            text = re.sub(re5,' ',text)\n",
    "            docs.append(text)           \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of either a) Stemming or b) Lemmatizing to find word roots. See [here](http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization) for a more detailed explination of the two. Right below, the stemmer is implemented, while two cells below, the lemmatizer is implemented. Make sure to choose which one to use before proceeding to Constructing the document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-dc42fdc20b24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# remove stop words from tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mstopped_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Stemming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-dc42fdc20b24>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# remove stop words from tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mstopped_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0men_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Stemming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenization\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # Removing stop words\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # Stemming \n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # stem token\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27 s\n"
     ]
    }
   ],
   "source": [
    "# We now employ the techniques as outline in the second link at the top - see **\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Tokenization\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # Removing stop words\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = get_stop_words('en')\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # Stemming \n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # stem token\n",
    "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "    \n",
    "    texts.append(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the document term matrix whereafter the fairly lengthy process of constructing the model takes place. Thus far the model seems be linear. With a single pass, the model takes just upward of a minute to execute, whereas for 5 passes, the model takes roughly 5.5 minutes. \n",
    "\n",
    "See the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.44 s\n"
     ]
    }
   ],
   "source": [
    "# Constructing a document-term matrix\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min\n"
     ]
    }
   ],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=7, id2word = dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "will enron meeting please group business forwarded also mark team jeff information like management trading office risk service contact conference time report need work sherri issue skilling access john provide market vince employee program regard plan member list opportunity call director serum system houston process discus legal committee question chairman review project presentation development manager m week site activity currently forward global energy know available executive company emission continue can help support next board product working regarding president research following new enrons monday mike meet area well delainey financial number\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 1: \n",
      "will power california utility contract rate cost state customer market energy issue price commission e also bill ferc pg order plant enron project permit facility generation proposal time based generator purchase service point need provide million make however capacity whether amount company supply term demand party made increase since case proposed edison option pipeline sale system result credit required information deal public agreement want year including charge additional gas transmission payment decision language regulatory release action value agency load believe without cpuc current part concern iso general event construction problem\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 2: \n",
      "agreement please pl credit will change attached master copy draft isda form document party letter need transaction legal sara trade review guaranty contract language company comment sent version confirmation shall also information swap send received request name section question date houston bank executed confirm revised trading account notice enron respect entity note term signed regard financial assignment order gisb counterparty schedule pending provision find energy regarding final doc requested forward ena approval sign security firm amendment new upon service execution page corporation list print first provide reference transfer termination contact\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 3: \n",
      "com enron smith street texas houston perlingiere dperlin legal department fax please sara phone debra message shackleton carol clair intended mark communication pm email john information error received susan mail richard recipient jeff st robert forwarded thanks confidential regard david bill call attached attachment paul net doc mary copy ect bob may aol notify scott s sender contact immediately distribution ca stephanie delete lynn william michael suite joe steve steven hou karen dissemination send mike frank privileged notified thank corp contain james draft u enron_development original review www ee mara\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 4: \n",
      "deal forwarded price month daily gas please volume number transwestern shall book will product contract term thanks storage total report day desk pipeline seller physical trader index curve fuel value need following trade transaction transport delivery west invoice date natural liquid period basis buyer sitara point monthly chris peak tagg fixed unit position change spread average test sell per file sale show location dp spreadsheet july power pipe zone purchase pricing field east quantity variance central le demand using eol system sheet data option hunter contractor pm negm correct s\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 5: \n",
      "said state energy california company davis power d governor billion electricity e year market price news last utility public stock million say week edu school first consumer false new share many high university time president water internet official assembly three much percent berkeley crisis month plan summer even buy world natural san federal industry one republican administration national evidence haas five sacramento money press just now department city little texas today every electric bush executive senate plant jones customer pacific gray deregulation tree come edison small way enough diego nbsp\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Topic 6: \n",
      "know will jeff thanks like just think want get need time dont call going please can make back forwarded work look good take let give also sure week still send see best message go well wanted last thing next love thought today help guy hope talk people one ben come really someone much getting tomorrow day great something tell might asked keep anything little night since morning trying meeting im dasovich check friday told find friend didnt right probably everyone looking mark forward said mike cant email around anyone office\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "time: 32.7 ms\n"
     ]
    }
   ],
   "source": [
    "num_topics = 7\n",
    "num_words = 90\n",
    "\n",
    "List = ldamodel.print_topics(num_topics, num_words)\n",
    "for i in range(0,len(List)):\n",
    "    word_list = re.sub(r'(.\\....\\*)|(\\+ .\\....\\*)', '',List[i][1])\n",
    "    print('Topic ' + str(i) + ': ' + '\\n' + str(word_list))\n",
    "    print('\\n' + '-'*100 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
